<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/rss-styles.xsl" type="text/xsl"?><rss version="2.0"><channel><title>Dereferenced</title><description>Ideas, traced to source; by Akshay Manglik.</description><link>https://akshaymanglik.com/</link><language>en-us</language><item><title>The Geometry of Attention</title><link>https://akshaymanglik.com/dereferenced/geometry-of-attention/</link><guid isPermaLink="true">https://akshaymanglik.com/dereferenced/geometry-of-attention/</guid><description>A geometric intuition for why attention mechanisms work so well across domainsâ€”from NLP to vision to audio.</description><pubDate>Fri, 26 Dec 2025 00:00:00 GMT</pubDate><category>transformers</category><category>attention</category><category>deep learning</category></item><item><title>Understanding GRPO</title><link>https://akshaymanglik.com/dereferenced/understanding-grpo/</link><guid isPermaLink="true">https://akshaymanglik.com/dereferenced/understanding-grpo/</guid><description>Group Relative Policy Optimization eliminates the learned value function. Here&apos;s why that matters for RL training.</description><pubDate>Sat, 15 Nov 2025 00:00:00 GMT</pubDate><category>rl</category><category>llm</category><category>training</category></item></channel></rss>